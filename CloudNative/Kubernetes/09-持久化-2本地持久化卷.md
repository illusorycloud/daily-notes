# 本地持久化卷

## 1. 概述

而在持久化存储领域，用户呼声最高的定制化需求，莫过于支持“本地”持久化存储了。

所以，Kubernetes 在 v1.10 之后，就逐渐依靠 PV、PVC 体系实现了这个特性。这个特性的名字叫作：**Local Persistent Volume**。

不过，首先需要明确的是，**Local Persistent Volume 并不适用于所有应用**。

> 适用范围：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用。

同时还要求：**使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力**。

> 否则就会出现数据丢失的情况



## 2. Local Persistent Volume

Local Persistent Volume 的设计，主要面临两个难点。

**第一个难点在于：如何把本地磁盘抽象成 PV。**

**你绝不应该把一个宿主机上的目录当作 PV 使用**。这是因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。而且，不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制。

一个 Local Persistent Volume 对应的存储介质，一定是一块额外挂载在宿主机的磁盘或者块设备（“额外”的意思是，它不应该是宿主机根目录所使用的主硬盘）。这个原则，我们可以称为**“一个 PV 一块盘”**。

**第二个难点在于：调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上呢？**

对于常规的 PV 来说，Kubernetes 都是先调度 Pod 到某个节点上，然后，再通过“两阶段处理”来“持久化”这台机器上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载。

可是，对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），必须是运维人员提前准备好的。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。

所以，这时候，调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。

这个原则，我们可以称为**“在调度的时候考虑 Volume 分布”**。在 Kubernetes 的调度器里，有一个叫作 VolumeBindingChecker 的过滤条件专门负责这个事情。在 Kubernetes v1.11 中，这个过滤条件已经默认开启了。



## 3. Demo

首先，在名叫 node-1 的宿主机上创建一个挂载点，比如 /mnt/disks；然后，用几个 RAM Disk 来模拟本地磁盘，如下所示：

```sh
# 在node-1上执行
$ mkdir /mnt/disks
$ for vol in vol1 vol2 vol3; do
    mkdir /mnt/disks/$vol
    mount -t tmpfs $vol /mnt/disks/$vol
done
```

然后定义 本地 PV

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/vol1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node-1
```

这个 PV 的定义里：local 字段，指定了它是一个 Local Persistent Volume；而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：/mnt/disks/vol1。

同时通过 nodeAffinity 字段指定 node-1 这个节点的名字，这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。

这正是 Kubernetes 实现**“在调度的时候就考虑 Volume 分布”**的主要方法。

接下来，我们就可以使用 kubect create 来创建这个 PV，如下所示：

```sh
$ kubectl create -f local-pv.yaml 
persistentvolume/example-pv created

$ kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY  STATUS      CLAIM             STORAGECLASS    REASON    AGE
example-pv   5Gi        RWO            Delete           Available                     local-storage             16s
```

然后定义 StorageClass

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

需要注意的是，在它的 provisioner 字段，我们指定的是 no-provisioner。这是**因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning**，所以它没办法在用户创建 PVC 的时候，就自动创建出对应的 PV。也就是说，我们前面创建 PV 的操作，是不可以省略的。

与此同时，这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性。它是 Local Persistent Volume 里一个非常重要的特性，即：**延迟绑定，将绑定操作推迟到调度的时候**。

> 绑定  Local Persistent Volume 其实就已经将 Pod 和 Node 绑定了，如果调度的时候某些原因导致Pod无法调度到该Node就会导致调度失败。
>
> 实际情况是该 Pod 除了这个  Local Persistent Volume外说不定还可以绑定其他的 PV，说不定就可以调度成功。
>
> 所以**将PV的绑定推迟到调度的时候执行，以避免绑定PV影响到Pod调度**。



然后定义一个 PVC：

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: example-local-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-storage
```

只需要 storageClassName 和前面创建的 PV 一致即可。

现在，我们来创建这个 PVC：

```sh
$ kubectl create -f local-pvc.yaml 
persistentvolumeclaim/example-local-claim created

$ kubectl get pvc
NAME                  STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS    AGE
example-local-claim   Pending                                       local-storage   7s
```

可以看到，尽管这个时候，Kubernetes 里已经存在了一个可以与 PVC 匹配的 PV，但这个 PVC 依然处于 Pending 状态，也就是等待绑定的状态。

然后，我们编写一个 Pod 来声明使用这个 PVC，如下所示：

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: example-pv-pod
spec:
  volumes:
    - name: example-pv-storage
      persistentVolumeClaim:
       claimName: example-local-claim
  containers:
    - name: example-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: example-pv-storage
```

而我们一旦使用 kubectl create 创建这个 Pod，就会发现，我们前面定义的 PVC，会立刻变成 Bound 状态，与前面定义的 PV 绑定在了一起，如下所示：

```sh
$ kubectl create -f local-pod.yaml 
pod/example-pv-pod created

$ kubectl get pvc
NAME                  STATUS    VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS    AGE
example-local-claim   Bound     example-pv   5Gi        RWO            local-storage   6h
```

也就是说，在我们创建的 Pod 进入调度器之后，“绑定”操作才开始进行。

这时候，我们可以尝试在这个 Pod 的 Volume 目录里，创建一个测试文件，比如：

```sh
$ kubectl exec -it example-pv-pod -- /bin/sh
# cd /usr/share/nginx/html
# touch test.txt
```

然后，登录到 node-1 这台机器上，查看一下它的 /mnt/disks/vol1 目录下的内容，你就可以看到刚刚创建的这个文件：

```sh
# 在node-1上
$ ls /mnt/disks/vol1
test.txt
```

而如果你重新创建这个 Pod 的话，就会发现，我们之前创建的测试文件，依然被保存在这个持久化 Volume 当中：

```sh
$ kubectl delete -f local-pod.yaml 

$ kubectl create -f local-pod.yaml 

$ kubectl exec -it example-pv-pod -- /bin/sh
# ls /usr/share/nginx/html
# touch test.txt
```

**需要注意的是，我们上面手动创建 PV 的方式，即 Static 的 PV 管理方式，在删除 PV 时需要按如下流程执行操作**：

* 1）删除使用这个 PV 的 Pod；
* 2）从宿主机移除本地磁盘（比如，umount 它）；
* 3）删除 PVC；
* 4）删除 PV。

如果不按照这个流程的话，这个 PV 的删除就会失败。

当然，由于上面这些创建 PV 和删除 PV 的操作比较繁琐，Kubernetes 其实提供了一个 Static Provisioner 来帮助你管理这些 PV。

> 具体文档看这里[ Static Provisioner](https://github.com/kubernetes-retired/external-storage/tree/master/local-volume#option-1-using-the-local-volume-static-provisioner)



## 4. 小结

* 1）创建 PV
  * 通过 local 表明是Local Persistent Volume
  * 通过 path 指定具体挂载目录
  * 通过 nodeAffinity 告诉调度器如何选择 Pod
* 2）创建  StorageClass
  * 通过 volumeBindingMode=WaitForFirstConsumer 将绑定步骤推迟到调度时执行，以免影响到Pod正常调度
* 3）创建 PVC
  * 比较简单，storageClassName 和 PV 一致即可
  * 该PVC创建后不会立刻绑定，说明 StorageClass 中的延迟调度策略是ok的
* 4）创建 Pod
  * 指定 Pod 使用上面的 PVC 即可
  * Pod 创建后 PVC就会随着Pod调度一起绑定
* 5）移除 PV，删除过程必须严格按照下面的顺序：
  * 删除使用这个 PV 的 Pod；
  * 从宿主机移除本地磁盘（比如，umount 它）；
  * 删除 PVC；
  * 删除 PV。